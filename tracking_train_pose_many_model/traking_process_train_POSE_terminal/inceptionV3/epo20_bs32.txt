
E:\CTU\LUAN_VAN_2023>python train_pose_inceptionV3_beta.py
[['./data_cropped/at_home/sit/sit_2047.jpg', 1], ['./data_cropped/at_home/sit/sit_807.jpg', 1], ['./data_cropped/at_home/lie/lie_wake_563.jpg', 0], ['./data_cropped/at_home/sit/sit_1525.jpg', 1], ['./data_cropped/at_home/stand/stand_1411.jpg', 2], ['./data_cropped/at_home/stand/stand_1713.jpg', 2], ['./data_cropped/at_home/minus/item_2125.jpg', 3], ['./data_cropped/at_home/lie/lie_wake_1140.jpg', 0], ['./data_cropped/at_home/lie/lie_wake_1502.jpg', 0], ['./data_cropped/at_home/stand/stand_1258.jpg', 2]]
Chuan bi doc anh tu folder:
scale raw pixel / 255.0
train test split
trainX shape:  (6464, 128, 128, 3)
testX shape:  (2020, 128, 128, 3)
trainY shape:  (6464, 4)
testY shape:  (2020,)
valX shape:  (1616, 128, 128, 3)
valY shape:  (1616, 4)
[INFO] compiling model...
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 inception_v3 (Functional)   (None, 2, 2, 2048)        21802784

 global_average_pooling2d (G  (None, 2048)             0
 lobalAveragePooling2D)

 dropout (Dropout)           (None, 2048)              0

 flatten (Flatten)           (None, 2048)              0

 dense (Dense)               (None, 4)                 8196

=================================================================
Total params: 21,810,980
Trainable params: 8,196
Non-trainable params: 21,802,784
_________________________________________________________________
None
bat dau fit model InceptionV3
2023-11-10 15:57:10.328347: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1270874112 exceeds 10% of free system memory.
Epoch 1/20
2023-11-10 15:57:23.664666: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.56GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2023-11-10 15:57:23.681765: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.85GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2023-11-10 15:57:23.700111: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.13GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
202/202 [==============================] - ETA: 0s - loss: 0.2322 - accuracy: 0.92312023-11-10 15:57:45.189197: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2023-11-10 15:57:45.331501: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.49GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2023-11-10 15:57:45.347913: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.78GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2023-11-10 15:57:45.365168: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.06GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
202/202 [==============================] - 34s 103ms/step - loss: 0.2322 - accuracy: 0.9231 - val_loss: 0.0220 - val_accuracy: 0.9932
Epoch 2/20
202/202 [==============================] - 17s 86ms/step - loss: 0.0460 - accuracy: 0.9858 - val_loss: 0.0157 - val_accuracy: 0.9938
Epoch 3/20
202/202 [==============================] - 17s 87ms/step - loss: 0.0244 - accuracy: 0.9916 - val_loss: 0.0123 - val_accuracy: 0.9975
Epoch 4/20
202/202 [==============================] - 18s 87ms/step - loss: 0.0193 - accuracy: 0.9938 - val_loss: 0.0133 - val_accuracy: 0.9975
Epoch 5/20
202/202 [==============================] - 18s 87ms/step - loss: 0.0172 - accuracy: 0.9941 - val_loss: 0.0195 - val_accuracy: 0.9938
Epoch 6/20
202/202 [==============================] - 18s 87ms/step - loss: 0.0115 - accuracy: 0.9968 - val_loss: 0.0121 - val_accuracy: 0.9975
Epoch 7/20
202/202 [==============================] - 18s 87ms/step - loss: 0.0088 - accuracy: 0.9969 - val_loss: 0.0162 - val_accuracy: 0.9963
Epoch 8/20
202/202 [==============================] - 18s 87ms/step - loss: 0.0120 - accuracy: 0.9964 - val_loss: 0.0130 - val_accuracy: 0.9981
Epoch 9/20
202/202 [==============================] - 18s 88ms/step - loss: 0.0091 - accuracy: 0.9972 - val_loss: 0.0132 - val_accuracy: 0.9988
Epoch 10/20
202/202 [==============================] - 18s 88ms/step - loss: 0.0069 - accuracy: 0.9980 - val_loss: 0.0109 - val_accuracy: 0.9988
Epoch 11/20
202/202 [==============================] - 18s 87ms/step - loss: 0.0128 - accuracy: 0.9957 - val_loss: 0.0169 - val_accuracy: 0.9963
Epoch 12/20
202/202 [==============================] - 18s 87ms/step - loss: 0.0092 - accuracy: 0.9974 - val_loss: 0.0199 - val_accuracy: 0.9950
Epoch 13/20
202/202 [==============================] - 18s 87ms/step - loss: 0.0071 - accuracy: 0.9983 - val_loss: 0.0170 - val_accuracy: 0.9969
Epoch 14/20
202/202 [==============================] - 18s 88ms/step - loss: 0.0065 - accuracy: 0.9975 - val_loss: 0.0148 - val_accuracy: 0.9963
Epoch 15/20
202/202 [==============================] - 18s 87ms/step - loss: 0.0105 - accuracy: 0.9977 - val_loss: 0.0173 - val_accuracy: 0.9969
Epoch 16/20
202/202 [==============================] - 18s 87ms/step - loss: 0.0066 - accuracy: 0.9977 - val_loss: 0.0202 - val_accuracy: 0.9957
Epoch 17/20
202/202 [==============================] - 18s 87ms/step - loss: 0.0093 - accuracy: 0.9972 - val_loss: 0.0239 - val_accuracy: 0.9963
Epoch 18/20
202/202 [==============================] - 18s 87ms/step - loss: 0.0090 - accuracy: 0.9969 - val_loss: 0.0306 - val_accuracy: 0.9944
Epoch 19/20
202/202 [==============================] - 18s 88ms/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.0204 - val_accuracy: 0.9963
Epoch 20/20
202/202 [==============================] - 18s 88ms/step - loss: 0.0109 - accuracy: 0.9969 - val_loss: 0.0218 - val_accuracy: 0.9969
new_model:   <keras.engine.sequential.Sequential object at 0x0000021A78FFFD60>
prepare save new_model:
bat dau kiem tra model:
63/64 [============================>.] - ETA: 0s2023-11-10 16:03:44.505655: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 928.81MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2023-11-10 16:03:44.520160: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.08GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2023-11-10 16:03:44.535171: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.26GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
64/64 [==============================] - 7s 81ms/step
E:\CTU\LUAN_VAN_2023\train_pose_inceptionV3_beta.py:154: UserWarning: FixedFormatter should only be used together with FixedLocator
  ax.set_xticklabels([''] + categories)
E:\CTU\LUAN_VAN_2023\train_pose_inceptionV3_beta.py:155: UserWarning: FixedFormatter should only be used together with FixedLocator
  ax.set_yticklabels([''] + categories)
Accuracy : 99.80%


F1 : 99.80%


Recall :99.80%


Precision : 99.80%


Time train inceptionV3:  6.16

E:\CTU\LUAN_VAN_2023>
