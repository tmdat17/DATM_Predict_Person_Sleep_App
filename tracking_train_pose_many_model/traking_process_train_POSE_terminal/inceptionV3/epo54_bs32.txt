
E:\CTU\LUAN_VAN_2023>python train_pose_inceptionV3_beta.py
[['./data_cropped/at_home/minus/item_2425.jpg', 3], ['./data_cropped/at_home/stand/stand_1487.jpg', 2], ['./data_cropped/at_home/minus/item_2270.jpg', 3], ['./data_cropped/at_home/sit/sit_1025.jpg', 1], ['./data_cropped/at_home/stand/stand_1901.jpg', 2], ['./data_cropped/at_home/lie/lie_wake_1526.jpg', 0], ['./data_cropped/at_home/minus/item_224.jpg', 3], ['./data_cropped/at_home/minus/item_1919.jpg', 3], ['./data_cropped/at_home/lie/lie_wake_1764.jpg', 0], ['./data_cropped/at_home/stand/stand_1313.jpg', 2]]
Chuan bi doc anh tu folder:
scale raw pixel / 255.0
train test split
trainX shape:  (6464, 128, 128, 3)
testX shape:  (2020, 128, 128, 3)
trainY shape:  (6464, 4)
testY shape:  (2020,)
valX shape:  (1616, 128, 128, 3)
valY shape:  (1616, 4)
[INFO] compiling model...
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 inception_v3 (Functional)   (None, 2, 2, 2048)        21802784

 global_average_pooling2d (G  (None, 2048)             0
 lobalAveragePooling2D)

 dropout (Dropout)           (None, 2048)              0

 flatten (Flatten)           (None, 2048)              0

 dense (Dense)               (None, 4)                 8196

=================================================================
Total params: 21,810,980
Trainable params: 8,196
Non-trainable params: 21,802,784
_________________________________________________________________
None
bat dau fit model InceptionV3
2023-11-10 17:32:14.752620: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1270874112 exceeds 10% of free system memory.
Epoch 1/54
2023-11-10 17:32:22.995878: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.56GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2023-11-10 17:32:23.015181: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.85GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2023-11-10 17:32:23.035971: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.13GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
202/202 [==============================] - ETA: 0s - loss: 0.2599 - accuracy: 0.92172023-11-10 17:32:43.917182: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2023-11-10 17:32:44.061227: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.49GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2023-11-10 17:32:44.078501: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.78GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2023-11-10 17:32:44.097716: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.06GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
202/202 [==============================] - 29s 105ms/step - loss: 0.2599 - accuracy: 0.9217 - val_loss: 0.0101 - val_accuracy: 0.9975
Epoch 2/54
202/202 [==============================] - 18s 89ms/step - loss: 0.0436 - accuracy: 0.9861 - val_loss: 0.0062 - val_accuracy: 0.9988
Epoch 3/54
202/202 [==============================] - 18s 89ms/step - loss: 0.0337 - accuracy: 0.9893 - val_loss: 0.0089 - val_accuracy: 0.9969
Epoch 4/54
202/202 [==============================] - 18s 89ms/step - loss: 0.0217 - accuracy: 0.9937 - val_loss: 0.0017 - val_accuracy: 1.0000
Epoch 5/54
202/202 [==============================] - 18s 89ms/step - loss: 0.0179 - accuracy: 0.9950 - val_loss: 0.0013 - val_accuracy: 1.0000
Epoch 6/54
202/202 [==============================] - 18s 89ms/step - loss: 0.0136 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 1.0000
Epoch 7/54
202/202 [==============================] - 18s 89ms/step - loss: 0.0131 - accuracy: 0.9957 - val_loss: 9.6594e-04 - val_accuracy: 1.0000
Epoch 8/54
202/202 [==============================] - 18s 90ms/step - loss: 0.0119 - accuracy: 0.9954 - val_loss: 0.0022 - val_accuracy: 1.0000
Epoch 9/54
202/202 [==============================] - 18s 90ms/step - loss: 0.0099 - accuracy: 0.9968 - val_loss: 0.0021 - val_accuracy: 0.9994
Epoch 10/54
202/202 [==============================] - 18s 89ms/step - loss: 0.0106 - accuracy: 0.9972 - val_loss: 8.0008e-04 - val_accuracy: 1.0000
Epoch 11/54
202/202 [==============================] - 18s 90ms/step - loss: 0.0082 - accuracy: 0.9969 - val_loss: 8.5317e-04 - val_accuracy: 1.0000
Epoch 12/54
202/202 [==============================] - 18s 89ms/step - loss: 0.0110 - accuracy: 0.9974 - val_loss: 1.7039e-04 - val_accuracy: 1.0000
Epoch 13/54
202/202 [==============================] - 18s 88ms/step - loss: 0.0091 - accuracy: 0.9971 - val_loss: 2.0207e-04 - val_accuracy: 1.0000
Epoch 14/54
202/202 [==============================] - 18s 88ms/step - loss: 0.0059 - accuracy: 0.9980 - val_loss: 1.7398e-04 - val_accuracy: 1.0000
Epoch 15/54
202/202 [==============================] - 18s 88ms/step - loss: 0.0064 - accuracy: 0.9975 - val_loss: 2.6359e-04 - val_accuracy: 1.0000
Epoch 16/54
202/202 [==============================] - 18s 88ms/step - loss: 0.0125 - accuracy: 0.9960 - val_loss: 1.8257e-04 - val_accuracy: 1.0000
Epoch 17/54
202/202 [==============================] - 18s 88ms/step - loss: 0.0087 - accuracy: 0.9974 - val_loss: 1.6687e-04 - val_accuracy: 1.0000
Epoch 18/54
202/202 [==============================] - 18s 88ms/step - loss: 0.0047 - accuracy: 0.9983 - val_loss: 5.3157e-04 - val_accuracy: 1.0000
Epoch 19/54
202/202 [==============================] - 18s 88ms/step - loss: 0.0080 - accuracy: 0.9977 - val_loss: 0.0104 - val_accuracy: 0.9963
Epoch 20/54
202/202 [==============================] - 18s 88ms/step - loss: 0.0073 - accuracy: 0.9978 - val_loss: 8.2200e-04 - val_accuracy: 1.0000
Epoch 21/54
202/202 [==============================] - 18s 88ms/step - loss: 0.0101 - accuracy: 0.9977 - val_loss: 0.0083 - val_accuracy: 0.9969
Epoch 22/54
202/202 [==============================] - 18s 87ms/step - loss: 0.0095 - accuracy: 0.9983 - val_loss: 0.0027 - val_accuracy: 0.9988
Epoch 23/54
202/202 [==============================] - 18s 87ms/step - loss: 0.0140 - accuracy: 0.9964 - val_loss: 2.6518e-04 - val_accuracy: 1.0000
Epoch 24/54
202/202 [==============================] - 18s 89ms/step - loss: 0.0125 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9994
Epoch 25/54
202/202 [==============================] - 18s 87ms/step - loss: 0.0101 - accuracy: 0.9974 - val_loss: 0.0010 - val_accuracy: 0.9994
Epoch 26/54
202/202 [==============================] - 18s 87ms/step - loss: 0.0073 - accuracy: 0.9983 - val_loss: 0.0026 - val_accuracy: 0.9994
Epoch 27/54
202/202 [==============================] - 18s 87ms/step - loss: 0.0069 - accuracy: 0.9977 - val_loss: 4.9045e-04 - val_accuracy: 1.0000
new_model:   <keras.engine.sequential.Sequential object at 0x00000293C4EEEB20>
prepare save new_model:
bat dau kiem tra model:
63/64 [============================>.] - ETA: 0s2023-11-10 17:40:45.756268: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 928.81MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2023-11-10 17:40:45.772851: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.08GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2023-11-10 17:40:45.789375: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.26GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
64/64 [==============================] - 7s 80ms/step
E:\CTU\LUAN_VAN_2023\train_pose_inceptionV3_beta.py:154: UserWarning: FixedFormatter should only be used together with FixedLocator
  ax.set_xticklabels([''] + categories)
E:\CTU\LUAN_VAN_2023\train_pose_inceptionV3_beta.py:155: UserWarning: FixedFormatter should only be used together with FixedLocator
  ax.set_yticklabels([''] + categories)
Accuracy : 99.65%


F1 : 99.65%


Recall :99.65%


Precision : 99.65%


Time train inceptionV3:  8.24

E:\CTU\LUAN_VAN_2023>
