
E:\CTU\LUAN_VAN_2023>python train_pose_inceptionV3_beta.py
[['./data_cropped/at_home/sit/sit_1723.jpg', 1], ['./data_cropped/at_home/stand/stand_2266.jpg', 2], ['./data_cropped/at_home/sit/sit_2308.jpg', 1], ['./data_cropped/at_home/lie/lie_wake_1418.jpg', 0], ['./data_cropped/at_home/minus/item_2564.jpg', 3], ['./data_cropped/at_home/stand/stand_1933.jpg', 2], ['./data_cropped/at_home/stand/stand_151.jpg', 2], ['./data_cropped/at_home/stand/stand_864.jpg', 2], ['./data_cropped/at_home/minus/item_789.jpg', 3], ['./data_cropped/at_home/minus/item_1561.jpg', 3]]
Chuan bi doc anh tu folder:
scale raw pixel / 255.0
train test split
trainX shape:  (6464, 128, 128, 3)
testX shape:  (2020, 128, 128, 3)
trainY shape:  (6464, 4)
testY shape:  (2020,)
valX shape:  (1616, 128, 128, 3)
valY shape:  (1616, 4)
[INFO] compiling model...
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 inception_v3 (Functional)   (None, 2, 2, 2048)        21802784

 global_average_pooling2d (G  (None, 2048)             0
 lobalAveragePooling2D)

 dropout (Dropout)           (None, 2048)              0

 flatten (Flatten)           (None, 2048)              0

 dense (Dense)               (None, 4)                 8196

=================================================================
Total params: 21,810,980
Trainable params: 8,196
Non-trainable params: 21,802,784
_________________________________________________________________
None
bat dau fit model InceptionV3
2023-11-10 14:59:45.864478: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1270874112 exceeds 10% of free system memory.
Epoch 1/20
2023-11-10 14:59:54.094312: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.48GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2023-11-10 14:59:54.110516: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.76GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2023-11-10 14:59:54.126730: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.04GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
538/539 [============================>.] - ETA: 0s - loss: 0.1853 - accuracy: 0.94492023-11-10 15:00:23.656621: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.46GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2023-11-10 15:00:23.672335: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.74GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2023-11-10 15:00:23.690164: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
539/539 [==============================] - 46s 72ms/step - loss: 0.1851 - accuracy: 0.9449 - val_loss: 0.0089 - val_accuracy: 0.9975
Epoch 2/20
539/539 [==============================] - 36s 66ms/step - loss: 0.0325 - accuracy: 0.9899 - val_loss: 0.0092 - val_accuracy: 0.9975
Epoch 3/20
539/539 [==============================] - 35s 66ms/step - loss: 0.0284 - accuracy: 0.9920 - val_loss: 0.0082 - val_accuracy: 0.9981
Epoch 4/20
539/539 [==============================] - 35s 66ms/step - loss: 0.0225 - accuracy: 0.9930 - val_loss: 0.0025 - val_accuracy: 0.9994
Epoch 5/20
539/539 [==============================] - 36s 66ms/step - loss: 0.0122 - accuracy: 0.9957 - val_loss: 0.0033 - val_accuracy: 0.9988
Epoch 6/20
539/539 [==============================] - 36s 66ms/step - loss: 0.0117 - accuracy: 0.9954 - val_loss: 0.0051 - val_accuracy: 0.9988
Epoch 7/20
539/539 [==============================] - 37s 68ms/step - loss: 0.0175 - accuracy: 0.9952 - val_loss: 0.0128 - val_accuracy: 0.9963
Epoch 8/20
539/539 [==============================] - 41s 76ms/step - loss: 0.0207 - accuracy: 0.9947 - val_loss: 0.0037 - val_accuracy: 0.9988
Epoch 9/20
539/539 [==============================] - 36s 66ms/step - loss: 0.0110 - accuracy: 0.9968 - val_loss: 0.0068 - val_accuracy: 0.9975
Epoch 10/20
539/539 [==============================] - 36s 66ms/step - loss: 0.0095 - accuracy: 0.9966 - val_loss: 0.0027 - val_accuracy: 0.9994
Epoch 11/20
539/539 [==============================] - 35s 66ms/step - loss: 0.0056 - accuracy: 0.9980 - val_loss: 0.0057 - val_accuracy: 0.9981
Epoch 12/20
539/539 [==============================] - 36s 66ms/step - loss: 0.0116 - accuracy: 0.9971 - val_loss: 0.0117 - val_accuracy: 0.9963
Epoch 13/20
539/539 [==============================] - 35s 66ms/step - loss: 0.0095 - accuracy: 0.9968 - val_loss: 0.0128 - val_accuracy: 0.9957
Epoch 14/20
539/539 [==============================] - 36s 66ms/step - loss: 0.0143 - accuracy: 0.9961 - val_loss: 0.0020 - val_accuracy: 0.9981
Epoch 15/20
539/539 [==============================] - 36s 66ms/step - loss: 0.0079 - accuracy: 0.9972 - val_loss: 0.0030 - val_accuracy: 0.9988
Epoch 16/20
539/539 [==============================] - 36s 66ms/step - loss: 0.0110 - accuracy: 0.9971 - val_loss: 0.0044 - val_accuracy: 0.9981
Epoch 17/20
539/539 [==============================] - 36s 66ms/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.0047 - val_accuracy: 0.9988
Epoch 18/20
539/539 [==============================] - 36s 66ms/step - loss: 0.0071 - accuracy: 0.9981 - val_loss: 0.0039 - val_accuracy: 0.9994
Epoch 19/20
539/539 [==============================] - 36s 66ms/step - loss: 0.0096 - accuracy: 0.9972 - val_loss: 0.0050 - val_accuracy: 0.9981
Epoch 20/20
539/539 [==============================] - 36s 66ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.0055 - val_accuracy: 0.9988
new_model:   <keras.engine.sequential.Sequential object at 0x00000200E90BEEE0>
prepare save new_model:
bat dau kiem tra model:
2023-11-10 15:12:38.667785: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1007.56MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2023-11-10 15:12:38.682638: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.16GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2023-11-10 15:12:38.699192: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.34GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2023-11-10 15:12:38.817842: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 840.50MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
64/64 [==============================] - 9s 77ms/step
E:\CTU\LUAN_VAN_2023\train_pose_inceptionV3_beta.py:154: UserWarning: FixedFormatter should only be used together with FixedLocator
  ax.set_xticklabels([''] + categories)
E:\CTU\LUAN_VAN_2023\train_pose_inceptionV3_beta.py:155: UserWarning: FixedFormatter should only be used together with FixedLocator
  ax.set_yticklabels([''] + categories)
Accuracy : 99.80%


F1 : 99.80%


Recall :99.80%


Precision : 99.80%


Time train inceptionV3:  12.18

E:\CTU\LUAN_VAN_2023>
