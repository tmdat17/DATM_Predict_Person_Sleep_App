
E:\CTU\LUAN_VAN_2023>python train_pose_denseNet121_beta.py
[['./data_cropped/at_home/sit/sit_479.jpg', 1], ['./data_cropped/at_home/minus/item_2079.jpg', 3], ['./data_cropped/at_home/lie/lie_wake_126.jpg', 0], ['./data_cropped/at_home/lie/lie_wake_776.jpg', 0], ['./data_cropped/at_home/sit/sit_2164.jpg', 1], ['./data_cropped/at_home/lie/lie_wake_675.jpg', 0], ['./data_cropped/at_home/lie/lie_wake_838.jpg', 0], ['./data_cropped/at_home/stand/stand_1637.jpg', 2], ['./data_cropped/at_home/stand/stand_626.jpg', 2], ['./data_cropped/at_home/sit/sit_608.jpg', 1]]
Chuan bi doc anh tu folder:
scale raw pixel / 255.0
train test split
trainX shape:  (6464, 128, 128, 3)
testX shape:  (2020, 128, 128, 3)
trainY shape:  (6464, 4)
testY shape:  (2020,)
valX shape:  (1616, 128, 128, 3)
valY shape:  (1616, 4)
[INFO] compiling model...
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 densenet121 (Functional)    (None, 4, 4, 1024)        7037504

 global_average_pooling2d (G  (None, 1024)             0
 lobalAveragePooling2D)

 dropout (Dropout)           (None, 1024)              0

 flatten (Flatten)           (None, 1024)              0

 dense (Dense)               (None, 4)                 4100

=================================================================
Total params: 7,041,604
Trainable params: 4,100
Non-trainable params: 7,037,504
_________________________________________________________________
None
bat dau fit model DenseNet121
2023-11-05 15:50:28.882372: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1270874112 exceeds 10% of free system memory.
Epoch 1/32
202/202 [==============================] - ETA: 0s - loss: 0.4420 - accuracy: 0.85502023-11-05 15:51:10.544227: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.18GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
202/202 [==============================] - 41s 151ms/step - loss: 0.4420 - accuracy: 0.8550 - val_loss: 0.0141 - val_accuracy: 0.9981
Epoch 2/32
202/202 [==============================] - 26s 127ms/step - loss: 0.0548 - accuracy: 0.9838 - val_loss: 0.0061 - val_accuracy: 1.0000
Epoch 3/32
202/202 [==============================] - 26s 127ms/step - loss: 0.0336 - accuracy: 0.9892 - val_loss: 0.0039 - val_accuracy: 1.0000
Epoch 4/32
202/202 [==============================] - 26s 127ms/step - loss: 0.0253 - accuracy: 0.9929 - val_loss: 0.0029 - val_accuracy: 1.0000
Epoch 5/32
202/202 [==============================] - 25s 126ms/step - loss: 0.0214 - accuracy: 0.9947 - val_loss: 0.0022 - val_accuracy: 1.0000
Epoch 6/32
202/202 [==============================] - 25s 126ms/step - loss: 0.0162 - accuracy: 0.9963 - val_loss: 0.0019 - val_accuracy: 1.0000
Epoch 7/32
202/202 [==============================] - 25s 125ms/step - loss: 0.0139 - accuracy: 0.9969 - val_loss: 0.0019 - val_accuracy: 1.0000
Epoch 8/32
202/202 [==============================] - 25s 126ms/step - loss: 0.0117 - accuracy: 0.9972 - val_loss: 0.0018 - val_accuracy: 1.0000
Epoch 9/32
202/202 [==============================] - 25s 125ms/step - loss: 0.0128 - accuracy: 0.9963 - val_loss: 0.0018 - val_accuracy: 1.0000
Epoch 10/32
202/202 [==============================] - 26s 127ms/step - loss: 0.0103 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 1.0000
Epoch 11/32
202/202 [==============================] - 27s 132ms/step - loss: 0.0128 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 1.0000
Epoch 12/32
202/202 [==============================] - 25s 125ms/step - loss: 0.0098 - accuracy: 0.9972 - val_loss: 8.1684e-04 - val_accuracy: 1.0000
Epoch 13/32
202/202 [==============================] - 26s 127ms/step - loss: 0.0106 - accuracy: 0.9969 - val_loss: 9.5867e-04 - val_accuracy: 1.0000
Epoch 14/32
202/202 [==============================] - 25s 126ms/step - loss: 0.0091 - accuracy: 0.9975 - val_loss: 7.4025e-04 - val_accuracy: 1.0000
Epoch 15/32
202/202 [==============================] - 25s 126ms/step - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.0014 - val_accuracy: 0.9988
Epoch 16/32
202/202 [==============================] - 25s 126ms/step - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 1.0000
Epoch 17/32
202/202 [==============================] - 26s 127ms/step - loss: 0.0091 - accuracy: 0.9968 - val_loss: 5.4342e-04 - val_accuracy: 1.0000
Epoch 18/32
202/202 [==============================] - 26s 126ms/step - loss: 0.0086 - accuracy: 0.9978 - val_loss: 5.0586e-04 - val_accuracy: 1.0000
Epoch 19/32
202/202 [==============================] - 25s 126ms/step - loss: 0.0098 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 1.0000
Epoch 20/32
202/202 [==============================] - 26s 127ms/step - loss: 0.0067 - accuracy: 0.9978 - val_loss: 4.0436e-04 - val_accuracy: 1.0000
Epoch 21/32
202/202 [==============================] - 25s 126ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 3.3543e-04 - val_accuracy: 1.0000
Epoch 22/32
202/202 [==============================] - 26s 127ms/step - loss: 0.0077 - accuracy: 0.9977 - val_loss: 7.8962e-04 - val_accuracy: 1.0000
Epoch 23/32
202/202 [==============================] - 25s 126ms/step - loss: 0.0082 - accuracy: 0.9978 - val_loss: 3.2913e-04 - val_accuracy: 1.0000
Epoch 24/32
202/202 [==============================] - 25s 126ms/step - loss: 0.0053 - accuracy: 0.9981 - val_loss: 3.0643e-04 - val_accuracy: 1.0000
Epoch 25/32
202/202 [==============================] - 25s 126ms/step - loss: 0.0100 - accuracy: 0.9964 - val_loss: 2.4937e-04 - val_accuracy: 1.0000
Epoch 26/32
202/202 [==============================] - 25s 125ms/step - loss: 0.0109 - accuracy: 0.9969 - val_loss: 3.8269e-04 - val_accuracy: 1.0000
Epoch 27/32
202/202 [==============================] - 25s 126ms/step - loss: 0.0063 - accuracy: 0.9983 - val_loss: 2.3964e-04 - val_accuracy: 1.0000
Epoch 28/32
202/202 [==============================] - 26s 126ms/step - loss: 0.0061 - accuracy: 0.9980 - val_loss: 2.2924e-04 - val_accuracy: 1.0000
Epoch 29/32
202/202 [==============================] - 26s 126ms/step - loss: 0.0065 - accuracy: 0.9981 - val_loss: 2.5490e-04 - val_accuracy: 1.0000
Epoch 30/32
202/202 [==============================] - 26s 127ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 3.0002e-04 - val_accuracy: 1.0000
Epoch 31/32
202/202 [==============================] - 25s 126ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 1.7776e-04 - val_accuracy: 1.0000
Epoch 32/32
202/202 [==============================] - 26s 127ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 1.5744e-04 - val_accuracy: 1.0000
new_model:   <keras.engine.sequential.Sequential object at 0x00000121BB960730>
prepare save new_model:
bat dau kiem tra model:
64/64 [==============================] - 10s 117ms/step
E:\CTU\LUAN_VAN_2023\train_pose_denseNet121_beta.py:154: UserWarning: FixedFormatter should only be used together with FixedLocator
  ax.set_xticklabels([''] + categories)
E:\CTU\LUAN_VAN_2023\train_pose_denseNet121_beta.py:155: UserWarning: FixedFormatter should only be used together with FixedLocator
  ax.set_yticklabels([''] + categories)
Accuracy : 99.85%


Recall :99.85%


Precision : 99.85%


F1 : 99.85%


Time train DenseNet121:  13.88

E:\CTU\LUAN_VAN_2023>
